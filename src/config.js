МИНИСТЕРСТВО ОБРАЗОВАНИЯ РЕСПУБЛИКИ БЕЛАРУСЬ БЕЛОРУССКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ
ФАКУЛЬТЕТ ПРИКЛАДНОЙ МАТЕМАТИКИ И ИНФОРМАТИКИ
Кафедра компьютерных технологий и систем




НЕЙРОННЫЕ СЕТИ В ЗАДАЧАХ ПРОГНОЗИРОВАНИЯ
Курсовой проект


Пунько Павла Александровича
студента 3 курса,
специальность «информатика»

Научный руководитель:

заведующий кафедрой КТС
Казаченок В. В.





Минск, 2020
Содержание
ВВЕДЕНИЕ	3
АНАЛИЗ ЗАДАЧ ПРОГНОЗИРОВАНИЯ	4
Постановка задачи прогнозирования	4
Методы решения задачи прогнозирования	5
АНАЛИЗ АППАРАТА НЕЙРОННЫХ СЕТЕЙ	8
Архитектура нейронных сетей	8
Использование нейронных сетей.	10
Обучение нейронной сети	11
АНАЛИЗ ПРОГРАММНЫХ СРЕДСТВ ДЛЯ РЕАЛИЗАЦИИ НЕЙРОННЫХ СЕТЕЙ	14
TensorFlow	14
Theano	15
Keras	16
ЭКСПЕРИМЕНТАЛЬНЫЙ АНАЛИЗ ЗАДАЧИ ПРОГНОЗИРОВАНИЯ	17
Построение рекуррентной сети с использованием TensorFlow	18
Построение перцептрона с использованием Keras	19
Результаты эксперимента	19
ЗАКЛЮЧЕНИЕ	24
СПИСОК ИСПОЛЬЗОВАНЫХ ИСТОЧНИКОВ	26
ПРИЛОЖЕНИЕ	27

 
ВВЕДЕНИЕ
Задача прогнозирования временных рядов была и остается актуальной, особенно в последнее время, когда стали доступны мощные средства сбора и обработки информации. Прогнозирование временных рядов является важной научно-технической проблемой, так как позволяет предсказать поведение различных факторов в экологических, экономических, социальных и иных системах. Развитие прогностики как науки в последние десятилетия привело к созданию множества моделей и методов, процедур, приемов прогнозирования, неравноценных по своему значению. По оценкам зарубежных и отечественных специалистов по прогностике уже насчитывается свыше ста методов прогнозирования, в связи с чем встает задача выбора методов, которые давали бы адекватные прогнозы для изучаемых процессов или систем. Жесткие статистические предположения о свойствах временных рядов зачастую ограничивают возможности классических методов прогнозирования. Применение нейронных сетей в данной задаче обусловлено наличием в большинстве временных рядов сложных закономерностей, не обнаруживаемых известными линейными методами. Поэтому целью своей работы я ставлю нахождение наиболее приемлемой для решения задач прогнозирования модели нейронной сети.
 
АНАЛИЗ ЗАДАЧ ПРОГНОЗИРОВАНИЯ
Прогнозирование – это предсказание будущих событий. Целью прогнозирования является уменьшение риска при принятии решений. Прогноз обычно получается ошибочным, но ошибка зависит от используемой прогнозирующей системы. Предоставляя прогнозу больше ресурсов, можно увеличить точность прогноза и уменьшить убытки, связанные с неопределенностью при принятии решений. Типичными приложениями техники прогноза являются предсказание цен на фондовой бирже, прогноз погоды, прогноз потребления электроэнергии, прогноз отказов технических систем и пр. 
В данной работе рассматривается задача прогнозирования временного ряда средствами нейронных сетей. Целью любого прогнозирования является создание модели, которая позволяет заглянуть в будущее и оценить тенденции в изменениях того или иного фактора. Качество прогноза в таком случае зависит от наличия предыстории изменяемого фактора, погрешностей измерения рассматриваемой величины и других факторов. 
Постановка задачи прогнозирования
Формально задача прогнозирования формулируется следующим образом: найти функцию f, позволяющую оценить значение переменной x в момент времени (t + d) по ее N предыдущим значениям, так чтобы x(t + d) = f (x(t), x(t -1),..., x(t - N +1)). Обычно d берется равным единице, то есть функция f прогнозирует следующее значение x. Временной ряд представляет собой последовательность наблюдаемых значений какого-либо признака, упорядоченных в неслучайные моменты времени. Отличием анализа временных рядов от анализа случайных выборок является предположение о равных промежутках времени между наблюдениями и их хронологический порядок. 

Методы решения задачи прогнозирования
При решении задачи прогнозирования необходимо идентифицировать переменные, которые будут прогнозироваться, временные параметры и степень точности прогноза. Часто при решении задач прогнозирования возникает необходимость предсказания не самой переменной, а изменений ее значений. Точность прогноза, требуемая для решения конкретной задачи, оказывает большое влияние на прогнозирующую систему. Ошибка прогноза зависит от используемой системы прогноза. Чем больше ресурсов имеет такая система, тем больше шансов получить более точный прогноз. Однако прогнозирование не может полностью уничтожить риски при принятии решений. Поэтому всегда учитывается возможная ошибка прогнозирования. Точность прогноза характеризуется ошибкой прогноза. 
Первый метод решения задачи прогнозирования носит статистический характер. При решении задач прогнозирования аналитику приходится принять решения относительно таких характеристик временного ряда как тренд, сезонная и циклическая компоненты, делать предположения о модели временного ряда – аддитивной, мультипликативной и др. Автоматического способа обнаружения трендов во временных рядах не существует. В то же время при изучении кривой, отражающей результаты наблюдений, аналитику трудно делать предположения относительно повторяемости формы кривой через равные промежутки времени. Общим недостатком статистических моделей является сложность выбора типа модели и подбора ее параметров. Все это существенно увеличивает субъективный вклад участников процесса анализа и прогнозирования временных рядов. Таким образом, результат анализа и прогнозирования временных рядов зависит как от квалификации аналитика в предметной отрасли, так и от его квалификации в методах анализа. 
Второй метод, который и будет применен в этой работе, использует нейросетевые технологии. Аппарат нейросети подразумевает минимальное участие аналитика в формировании модели временного ряда, так как способность нейросетевых моделей к обучению позволяет выявить скрытые взаимосвязи и закономерности между данными, а алгоритмы обучения адаптируют весовые коэффициенты в соответствии со структурой данных, представленных для обучения. Использование аппарата нейросети для прогнозирования временных рядов с заключается в формировании нейросетью определенной структуры, в ее параметрической настройке на основе поведения исследуемой системы в заранее известные моменты времени, в предсказании будущего поведения системы по ее предыстории. Выбор структуры нейронной сети обусловливается спецификой и сложностью решаемой задачи. Для решения некоторых типов задач разработаны оптимальные конфигурации. 
 Качество обученной нейросети оценивается на контрольной выборке, также выделенной из множества исходных данных. Обучающая и контрольная выборка не пересекаются и, как правило, в множестве исходных данных хронологически следуют друг за другом. Если на контрольной выборке значение ошибки находится в допустимых пределах, то настройка нейросети считается завершенной, а нейронная сеть готовой для решения задачи прогнозирования. 
При построении модели прогнозирования часто приходится выполнять некоторое предварительное преобразование данных с целью удовлетворения ряду известных требований. Это касается как прогнозируемой величины, так и обучающей и контрольной выборок. Основное требование к прогнозируемой величины заключается в том, чтобы было возможно прогнозировать будущие значения временного ряда. Как правило, в зависимости от задачи, приходится прибегать к таким методам по преобразованию входных данных, которые позволяют правильно судить о закономерностях и особенностях в данных, отражающих их качественные характеристики. Часто встает задача отображения данных в пространство меньшей размерности. Такая свертка данных приводит к устранению избыточности в данных и сокращению времени обучения нейросети. Одним из главных требований ко временному ряду является его стационарность, состоящая в том, что распределение его значений является инвариантным относительно момента времени, для которого оно построено. Для характеристики стационарности используется то, что для двух выборок, построенных в разные моменты времени, закон распределения должен оставаться тем же. Если элемент рассматриваемых временных рядов многомерен, следует анализировать каждый компонент xi элемента временного ряда и проверить, что он имеет равномерное распределение на отрезке [mi - σi , mi + σi,], где mi - математическое ожидание признака xi , σi – ее среднеквадратичное отклонение. Обычно это достигается путем следующего преобразования:
X^*=  (X-M{x})/(σ{x})   (1)
Далее обучение происходит с использованием полученной выборки X^*
 
АНАЛИЗ АППАРАТА НЕЙРОННЫХ СЕТЕЙ
Нейросетевые методы обработки информации стали использоваться несколько десятилетий назад. С течением времени интерес к нейросетевым технологиям то ослабевал, то вновь возрождался. Такое непостоянство напрямую связано с практическими результатами проводимых исследований. На сегодняшний день возможности нейросетевых технологий используются во многих отраслях науки, начиная от медицины и астрономии, заканчивая информатикой и экономикой. Способность нейронной сети к разносторонней обработке информации следует из ее способности к обобщению и выделению скрытых зависимостей между входными и выходными данными. Большим преимуществом нейронных сетей является то, что они способны к обучению и обобщению накопленных знаний.
Архитектура нейронных сетей
Искусственные нейронные сети используют биологическую аналогию с нейронами, которые находятся внутри головного мозга. Биологические нейроны устроены очень сложно, но упрощенно основные компоненты следующие: у нейрона есть ядро, в котором накапливается электрический заряд. Заряд поступает через отростки нейронов, которые называются дендриты. Через эти отростки приходят сигналы от других нейронов. После того как внутри ядра накопился определенный объем заряда, нейрон срабатывает и выдает электрический сигнал на выходной отросток, который называется аксон. Аксон в свою очередь прикрепляется к входным отросткам (дендритам) других нейронов. Прикрепление выполняется через так называемые синапсы, которые могут изменять передаваемый сигнал. Если в синапсе сигнал увеличивается, то такой синапс называется возбуждающим, а если сигнал уменьшаются, то синапс называется тормозящим. 
 МакКаллок и Питс в середине прошлого века предложили модель искусственного нейрона по аналогии с биологическим нейроном. У искусственного нейрона есть несколько входов – это аналог дендритов. На эти входы подаются входные сигналы. У искусственного нейрона один выход через который выдается выходной сигнал по аналогии с аксоном в биологическом нейроне. Каждому входу назначается некоторый вес – это аналог синапса. Большой положительный вес усиливает сигнал как возбуждающее синапс, а отрицательный вес – ослабляет входной сигнал как тормозящий синапс. Входной сигнал нейрону, поступающий на каждый вход умножается на вес, затем все это складывается и передается на вход некоторой нелинейной функции, которые называются функция активации. Функция активации определяет срабатывает нейроны или нет. В модели МакКаллока-Питса в качестве функции активации использовалась так называемая функция Хевисайда.
Если входной сигнал меньше заданного уровня, то на выходе нейрона подается ноль, а после того как сигнал достиг некоторого уровня, то нейрон сразу выдает единицу. Также возможно использовать другие функции активации. Часто используют сигмоиды. Данные функции выполняют не такое резкое переключение, а более гладкое. В качестве функции активации можно применять логистическую функцию, а также гиперболический тангенс.
Мак-Каллок и Питс в своей работе также предложили подход объединение искусственных нейронов в нейронные сети (Рисунок 1). Для этого выходной сигнал единицу нейроны передается на вход другому нейрону. Сеть может состоять из большого количества нейронов, объединенные в слои. Входной слой нейронов получают сигналы от внешнего мира, выходной слой выдает сигналы во внешний мир. Скрытый слой нейронной сети получает входные сигналы от нейронов входного слоя и выдает сигнал на выходной слой. Слой называется скрытым, так как то, что в нем происходит, не видно из внешней среды.
 
Рисунок 1. Модель нейронной сети МакКаллока-Питса
Есть два основных типа нейронной сети: сеть с прямым распространением и рекуррентная сеть. В сети с прямым распространением сигнала запрещены циклы. Сигнал которые поступают на входной слой, передается на скрытый слой, затем на выходной слой и во внешнюю среду. Другой тип сетей – рекуррентная сеть. В этих сетях возможны циклы. Это значит, что выходной сигнал от нейрона может поступать на вход к этому нейрону, к другим нейронам этого слоя или даже к нейронам предыдущего слоя. Рекуррентная сеть можно представить в виде сети с прямым распространение сигнала развернутой во времени.  Нейронные сети могут включать не один скрытый слой, а несколько. Сети, у которых больше чем один скрытый слой называются глубокими нейронными сетями.
Использование нейронных сетей.
Нейронные сети используются для решения сложных задач, которые требуют аналитических вычислений подобных тем, что делает человеческий мозг. Самыми распространенными применениями нейронных сетей является:
1. Классификация — распределение данных по параметрам. Например, на вход дается набор людей и нужно решить, кому из них давать кредит, а кому нет. Эту работу может сделать нейронная сеть, анализируя такую информацию как: возраст, платежеспособность, кредитная история и тд.
2. Предсказание — возможность предсказывать следующий шаг. Например, рост или падение акций, основываясь на ситуации на фондовом рынке.
3. Распознавание — в настоящее время, самое широкое применение нейронных сетей. Используется в Google, когда вы ищете фото или в камерах телефонов, когда оно определяет положение вашего лица и выделяет его и многое другое.
Обучение нейронной сети
Обучение нейронной сети – это процесс, в котором параметры нейронной сети настраиваются посредством моделирования среды, в которую эта сеть встроена. Тип обучения определяется способом подстройки параметров. Различают алгоритмы обучения с учителем и без учителя.
Процесс обучения с учителем представляет собой предъявление сети выборки обучающих примеров. Каждый образец подается на входы сети, затем проходит обработку внутри структуры нейронной сети, вычисляется выходной сигнал сети, который сравнивается с соответствующим значением целевого вектора, представляющего собой требуемый выход сети. Затем по определенному правилу вычисляется ошибка, и происходит изменение весовых коэффициентов связей внутри сети в зависимости от выбранного алгоритма. Векторы обучающего множества предъявляются последовательно, вычисляются ошибки и веса подстраиваются для каждого вектора до тех пор, пока ошибка по всему обучающему массиву не достигнет приемлемо низкого уровня.
При обучении без учителя обучающее множество состоит лишь из входных векторов. Обучающий алгоритм подстраивает веса сети так, чтобы получались согласованные выходные векторы, т.е. чтобы предъявление достаточно близких входных векторов давало одинаковые выходы. Процесс обучения, следовательно, выделяет статистические свойства обучающего множества и группирует сходные векторы в классы. Предъявление на вход вектора из данного класса даст определенный выходной вектор, но до обучения невозможно предсказать, какой выход будет производиться данным классом входных векторов. Следовательно, выходы подобной сети должны трансформироваться в некоторую понятную форму, обусловленную процессом обучения. Это не является серьезной проблемой. Обычно не сложно идентифицировать связь между входом и выходом, установленную сетью.
Для обучения нейронных сетей без учителя применяются сигнальные метод обучения Хебба и Ойа.
Математически процесс обучения можно описать следующим  образом. В процессе функционирования нейронная сеть формирует выходной сигнал Y, реализуя некоторую функцию Y = G(X). Если архитектура сети задана, то вид функции G определяется значениями синаптических весов и смещенной сети.
Пусть решением некоторой задачи является функция Y = F(X), заданная параметрами входных-выходных данных (X1, Y1), (X2, Y2), …, (XN, YN), для которых Yk = F(Xk) (k = 1, 2, …, N).
Обучение состоит в поиске (синтезе) функции G, близкой к F в смысле некоторой функции ошибки E.
Если выбрано множество обучающих примеров – пар (Xk, Yk) (где k = 1, 2, …, N) и способ вычисления функции ошибки E, то обучение нейронной сети превращается в задачу многомерной оптимизации, имеющую очень большую размерность, при этом, поскольку функция E может иметь произвольный вид обучение в общем случае – многоэкстремальная невыпуклая задача оптимизации.
Рассмотрим одну из классических архитектур нейронной сети –многослойный перцептрон с сисгмоидой в качестве функции активации на каждом нейроне и средней квадратичной ошибкой в качестве функции ошибки.
Такая модель, как и большинство моделей, разделяет обучение на две стадии – прямое и обратное распространение, которые чередуются. Пусть на вход подается вектор Х обучающей выборки и на выходе получаем вектор Y правильных ответов. Тогда прямое распространение выглядит как последовательное скалярное произведение выходов на веса, а затем вычисление результата функции активации от этого произведения на каждом слое:
{█(h_1^((j))=f(w_11^((j-1) ) x_1+w_12^((j-1) ) x_2+⋯+w_(1n_j)^((j-1) ) x_n)@h_2^((j))=f(w_21^((j-1) ) x_1+w_22^((j-1) ) x_2+⋯+w_(2n_j)^((j-1) ) x_n)@…@h_3^((j) )=f(w_31^((j-1) ) x_1+w_32^((j-1) ) x_2+⋯+w_(3n_j)^((j-1) ) x_n )@ )┤
j=1,2,…,k. 
 k-число слоев,n_j-число нейронов на j-ом слое,f-сигмоида
Дойдя до последнего слоя, вычисляется функция ошибки:
E=1/2 〖||Y-h^k ||〗^2  
Далее для корректировки весов используется градиентный спуск, который минимизирует функцию ошибки. 
w_kl^((j) )=w_kl^((j) )-(d E)/(dw_kl^((j) ) )
 
АНАЛИЗ ПРОГРАММНЫХ СРЕДСТВ ДЛЯ РЕАЛИЗАЦИИ НЕЙРОННЫХ СЕТЕЙ 
В последние годы интенсивно развиваются методы, технологии и программные средства для реализации, обучения нейронных сетей. Связано это с огромным количеством разнообразных накопленных данных, которые можно использовать для обучения нейронной сети. В таких условиях методы, разработанные еще в прошлом веке, находят свое экспериментальное применение. Развитию нейронных сетей так же способствует увеличение вычислительной мощности современных компьютеров, что значительно ускоряет процесс обучения нейронной сети. Огромным рывком в этой сфере стало использование GPU в качестве мощных и многочисленных вычислителей для обучения нейронной сети. С развитием такого языка, как Python, его возможностей векторных арифметических операций, были созданы специальные библиотеки машинного обучения и нейронных сетей, такие как TensorFlow, Theano и фреймворк на их основе – Keras. Все они реализуют сложные вычисления с низкоуровневым языком программирования С++, который встроен под коробку Python, за счет чего в разы ускоряют вычисления. Рассмотрим каждый из них поподробнее.
TensorFlow 
Tensorflow— довольно молодой фреймворк для глубокого машинного обучения, разрабатываемый в Google Brain. Долгое время фреймворк разрабатывался в закрытом режиме под названием DistBelief, но после глобального рефакторинга 9 ноября 2015 года был выпущен в открытый доступ. За год с небольшим TF дорос до версии 1.0, обрел интеграцию с keras, стал значительно быстрее и получил поддержку мобильных платформ. В последнее время фреймворк развивается еще и в сторону классических методов. Мы будем рассматривать только Python API, хотя это не единственный вариант — также существуют интерфейсы для C++ и мобильных платформ.
Работа c Tensorflow строится вокруг построения и выполнения графа вычислений. Граф вычислений — это конструкция, которая описывает то, каким образом будут проводиться вычисления. В классическом императивном программировании мы пишем код, который выполняется построчно. В Tensorflow привычный императивный подход к программированию необходим только для каких-то вспомогательных целей. Основа Tensorflow — это создание структуры, задающей порядок вычислений. Программы естественным образом структурируются на две части — составление графа вычислений и выполнение вычислений в созданных структурах.
В Tensorflow граф состоит из плейсхолдеров, переменных и операций. Из этих элементов можно собрать граф, в котором будут вычисляться тензоры. Тензоры — многомерные массивы, они служат «топливом» для графа. Тензором может быть как отдельное число, вектор признаков из решаемой задачи или изображение, так и целый батч описаний объектов или массив из изображений. Вместо одного объекта мы можем передать в граф массив объектов и для него будет вычислен массив ответов. Работа Tensorflow с тензорами похожа на то, как обрабатывает массивы numpy, в функциях которого можно указать ось массива, относительно которой будет выполняться вычисление.
Theano
Theano - проект с открытым исходным кодом, выпущенный по лицензии BSD и разработанный LISA группа в университете Монреаля, Квебек, Канада.
В основе Theano лежит компилятор математических выражений в Python. Он знает, как взять структуры и превратить их в очень эффективный код, который использует NumPy, эффективные нативные библиотеки, такие как BLAS и нативный код (C ++) для максимально быстрой работы на процессорах или графических процессорах.
Theano использует множество умных оптимизаций кода, чтобы выжать как можно больше производительности из вашего оборудования. Theano был специально разработан для обработки типов вычислений, необходимых для алгоритмов больших нейронных сетей, используемых в Deep Learning. Это была одна из первых библиотек в своем роде (разработка началась в 2007 году), и она считается отраслевым стандартом для исследований и разработок Deep Learning.
Keras
Keras – это высокоуровневый API TensorFlow 2.0 – доступный, высокопроизводительный интерфейс для решения задач машинного обучения с упором на современное глубокое обучение. Он предоставляет необходимые абстракции и строительные блоки для разработки и доставки решений машинного обучения с высокой скоростью итераций.
Эта библиотека содержит многочисленные реализации широко применяемых строительных блоков нейронных сетей, таких как слои, целевые и передаточные функции, оптимизаторы, и множество инструментов для упрощения работы с изображениями и текстом. 
Keras дает инженерам и исследователям возможность в полной мере использовать возможности масштабируемости и кроссплатформенности TensorFlow 2.0. Keras запускается на TPU или на больших кластерах графических процессоров, а также можно экспортировать свои модели Keras для работы в браузере или на мобильном устройстве. 
Беря во внимание все преимущества и недостатки описанных фреймворков, в качестве технологии для создания, обучения и использования нейронной сети для нашей задачи прогнозирования применим два фреймворка: TensorFlow и Keras. С помощью фреймворка TensorFlow построим рекуррентную нейронную сеть (LSTM), которая по своему строению подходит для решения задачи прогнозирования временных рядов. Также с помощью Keras построим многослойный перцептрон с автоматическим подбором гиперпараметров для сравнения его работы с работой рекуррентной сети.
ЭКСПЕРИМЕНТАЛЬНЫЙ АНАЛИЗ ЗАДАЧИ ПРОГНОЗИРОВАНИЯ
В качестве типичной задачи прогнозирования временных рядов возьмем задачу прогнозирования температуры. Для обучающей, валидационной и тестовой выборки будем использовать временные последовательности данных о погоде, записанные на гидрометеорологической станции в Институте биогеохимии им. Макса Планка.
В этот набор данных включены замеры 14 различных метеорологических показателей (таких, как температура воздуха, атмосферное давление, влажность), выполняющиеся каждые 10 минут начиная с 2003 года. Для экономии времени и используемой памяти будут использоваться данные, охватывающие период с 2009 по 2016 год. Этот раздел набора данных был подготовлен Франсуа Шолле (François Chollet) для его книги «Глубокое обучение на Python».
Период записи наблюдения составляет 10 минут. Таким образом, в течение одного часа у нас будет 6 наблюдений. В свою очередь за сутки накапливается 144 (6x24) наблюдения.
Прогнозировать будем температуру, которая будет на следующем измерении, то есть через 10 минут. Этот прогноз делается на основе имеющихся у нас данных за 5 наблюдений. Также подобный эксперимент проведем для 20, 50, 100 предыдущих наблюдений.
На первом этапе производится выборка данных начиная с 2009 года. Первые 300 000 строк данных будут использоваться для обучения модели, оставшиеся – для её валидации (проверки). В этом случае объём обучающих данных составляет примерно 2100 дней.
Как ранее оговаривалось, для обучающей, валидационной и тестовой выборки будем использовать временные последовательности данных о погоде, записанные на гидрометеорологической станции в Институте биогеохимии им. Макса Планка. Данные приводятся к стандартному нормальному распределению путем вычитания матожидания обучающей выборки и делением результата на дисперсию обучающей выборки. В дальнейшем это позволит ускорить процесс обучения.
Далее с помощью функции univariate_data (см. Приложение) будет производиться формирование массива временных последовательностей соответствующей длины для выборок и соответствующий ему массив правильных ответов.
Построение рекуррентной сети с использованием TensorFlow
TensorFlow использует технологии пакетных и перетасованных данных в своих алгоритмах, поэтому пакетируем их с помощью функции batch и перестасовываем с помощью функции shuffle. Таким образом сеть будет менее подвержена переобучению.
В качестве модели сети используем рекуррентную сеть с LSTM слоем (слоем с долгой краткосрочной памятью). Такая сеть подходит для временных последовательностей благодаря тому, что использует цикл for для итерации по упорядоченной по времени последовательности, храня при этом во внутреннем состоянии закодированную информацию о шагах, которые он уже видел. Данные, n-мерный вектор, подаются на вход рекуррентного 100-узлового слоя, затем проходят один 50-узловой последовательный слой и результат подается на выходной нейрон.
В качестве метода, минимизирующего функцию ошибки, используем, оптимизатор реализующий алгоритм Адама. Оптимизация Адама - это метод стохастического градиентного спуска, основанный на адаптивной оценке моментов первого и второго порядка. Согласно Kingma et al., 2014, этот метод «эффективен с вычислительной точки зрения, требует небольшого объема памяти, инвариантен к диагональному масштабированию градиентов и хорошо подходит для задач, больших с точки зрения данных / параметров». Сама функция ошибки – функция абсолютной средней ошибки. 
Нейросеть обучается на тестовой выборке 10 эпох, в каждой эпохе по 200 итераций. 
Построение перцептрона с использованием Keras
Фреймворк Keras предоставляет технологию Keras Tuner, которая позволяет оптимизировать такие гиперпараметры обучения нейронной сети, как:
	Количество слоев нейронной сети;
	Количество нейронов в каждом слое;
	Функции активации, которые используются в слоях;
	Тип оптимизатора при обучении нейронной сети;
	Количество эпох обучения.
Оптимизировать будем количество по количеству нейронов на скрытых слоях, по функции активации и по алгоритму минимизации функции ошибки. Количество слоев будем варьировать от 1 до 5. Количество нейронов на каждом скрытом слое зададим от 10n до 1000n, где n – номер слоя, считая с конца сети. В качестве функций активации рассмотрим Сигмоиду, ReLu, Elu, гиперболический тангенс. В качестве алгоритма оптимизации будем выбирать из градиентного спуска, оптимизации Адама и оптимизации RMSProp. В каждой эпохе будет 2400 итераций.
Результаты эксперимента
Результаты обучения, выраженные занчением средней абсолютной ошибки на тестовой и валидационной выборке представлены в Таблице 1.
 
Таблица 1. Результаты обучения рекуррентной сети.
Число предыдущих значений для предсказания	Время обучения, сек	Ошибка на валидационной выборке	Ошибка на тренировочной выборке
5	40	0,0151	0,0167
20	130	0,0150	0,0158
50	300	0.0148	0.0155
100	600	0.0143	0.0153

Ниже на графиках представлены значения ошибки на валидационной и тренировочной выборках в процессе обучения:
  
Рисунок 2. Ошибка на валидационной и тренировочной выборках на 5 и 20 предыдущих значениях в процессе обучения.
 
  
Рисунок 3. Ошибка на валидационной и тренировочной выборках на 50 и 100 предыдущих значениях в процессе обучения.
Во избежании переобучения, когда ошибка не уменьшается на тестовой выборке и уменьшается на тренировочной, количество эпох было выбрано равным 10. 
Как видно из результатов обучения, а именно значения ошибки на валидационной(тестовой) выборке, точность прогнозирования напрямую зависит от количества входных параметров, то есть количества предыдущих значений. Использование рекуррентной нейронной сети позволило приблизить точность прогнозирования до 0.0143 при использовании 100 предыдущих значений. При продолжении тенденции увеличения количества предыдущих значений, основываясь на данных эксперимента, можно утверждать о росте точности и уменьшении погрешности прогнозирования. Более подробные графики прогнозирования можно найти в Приложении.
 
Многослойный перцептрон с оптимизацией параметров показал результаты ошибки, в разы превышающей показатели рекуррентной сети. Минимальная ошибка была достигнута на следующих параметрах:
	Функция активации ReLu.
	2 скрытых слоя
	Количество нейронов на первом скрытом слое – 970
	Количество нейронов на втором скрытом слое – 1070
	Алгоритм – оптимизация Адама
Результаты обучения, выраженные значением средней абсолютной ошибки на тестовой и валидационной выборке представлены в Таблице 2:
Таблица 2. Результаты обучения многослойного перцептрона.
Число предыдущих значений для предсказания	Время обучения, сек	Ошибка на валидационной выборке	Ошибка на тренировочной выборке
5	50	0,1483	0,1404
20	50	0.1578	0.1362
50	10	0.1573	0.1711
100	125	0,1612	0,1541

Ниже на графиках представлены значения ошибки на валидационной и тренировочной выборках в процессе обучения:
  
Рисунок 4. Ошибка на валидационной и тренировочной выборках на 5 и 20 предыдущих значениях в процессе обучения.
  
Рисунок 5. Ошибка на валидационной и тренировочной выборках на 50 и 100 предыдущих значениях в процессе обучения.
При использовании подобной модели нейронной сети получили неудовлетворительные результаты с довольно большой и неприемлемой погрешностью. Несмотря на варьирование в экспериментальных целях числа эпох и итераций, не удалось добиться приемлемой тенденции уменьшения погрешности ошибки на валидационной выборке. Меньше погрешность получить можно путем увеличения числа эпох, но это приведет к переобучению сети, и она станет непригодной для прогнозирования на новых данных. Из этого можно заключить, что модель многослойного перцептрона не подходит для задач прогнозирования. Результаты прогнозирования для таких сетей приводиться не будут.
ЗАКЛЮЧЕНИЕ
В ходе данной работы был рассмотрен такой класс задач, как задачи прогнозирования временных рядов. Определена общая постановка данной задачи, области и сферы применения, такие, как экономика, медицина, информатика и метеорология. Показана важность данных задач. Рассмотрены возможные методы решения, среди которых был выделен метод решения с использованием инструмента нейронных сетей. В связи с этим, рассмотрен аппарат нейронных сетей.  Исследован принцип работы нейронных сетей, основные характеристики, этапы реализации. Исследованы алгоритмы, которые используются аппаратом нейронных сетей для эффективной и корректной работы. Исследованы понятия обучения нейронной сети и понятие гиперпараметров нейронной сети. Для практического применения проведен анализ программных средств для реализации нейронных сетей. Среди них выделены модули Keras и TensorFlow, написанные на языке Python. Поэтому именно Python и его модули были выбран в качестве средств для экспериментального анализа задач прогнозирования. Был проведен эксперимент, в ходе которого реализованы и обучены рекуррентная нейронная сеть и многослойный перцептрон. Данными для эксперимента были временные ряды показаний температуры воздуха. Результаты работы и предсказаний обоих нейронных сетей были проанализированы, и, как и предполагалось на основании проведенных ранее научных исследований, наиболее эффективной для решения задачи прогнозирования временных радов оказалась рекуррентная нейронная сеть. Как видно из полученных результатов, выбор рекуррентной сети оправдан для задач прогнозирования. При обучении соизмеримых по размерности нейронных сетей, несмотря на большее время обучения у рекуррентной сети, лучший результат при решении задачи прогнозирования временных рядов дала именно рекуррентная сеть. Это свидетельствует о высокой эффективности такой архитектуры сети. Беря во внимание среднее абсолютное значение ошибки при использовании рекуррентной сети, можно утверждать о высокой эффективности метода нейронных сетей для решения задачи прогнозирования временных рядов.

 
СПИСОК ИСПОЛЬЗОВАНЫХ ИСТОЧНИКОВ
	Галушкин А.И. Нейронные сети: основы теории. - М.:, 2010. - 496 с.
	Осовский С. Нейронные сети для обработки информации / Пер. с польского И.Д. Рудинского. – М.: Финансы и статистика, 2002. – 344 с.
	Рашид, Тарик. Создаем нейронную сеть. / Пер. с англ. — СПб. : ООО “Альфа-книга”, 2017. — 272 с. 
	Розенблатт Ф. Принципы нейродинамики. Перцептроны и теория механизмов мозга. –М.: Мир, 1965. – 478 с.
	Уоссермен Ф. Нейрокомпьютерная техника: теория и практика / Пер. с английского Ю.А. Зуев. – М.: Мир, 1992.
	Хайкин C. Нейронные сети: полный курс. – 2-е изд.– М.: "Вильямс", 2006.
	Aurellen Geron. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems – 1-ое издание – C.:O’REILLY, 2019. – 801 с.
 
